# Forecasting

```{r child = 'pre-chapter-script.Rmd'} 
```


In this chapter, we discuss three methods for forecasting time series. 

1. Find a lead variable. That is, find a variable that is highly correlated with the variable that you wish to forecast after a lag. An example is that new housing construction permits are a lead variable for variables related to house construction, such as home construction materials.
2. Use previous patterns from similar time series to inform your forecast for the time series under consideration. The example we will consider is the Bass model, which is a model for the number of units sold of a new consumer item.
3. Extrapolate from current trends in the time series. We will consider exponential smoothing and the Holt-Winters method.

## Lead Variables

Consider the `housing` data set in the `tswrdata` package. This consists of the number of houses approved for construction together with dollars spent on housing construction for that quarter, adjusted for inflation to 2004 Australian dollars. 

We start by plotting approvals and activity on the same plot.

```{r}
hh <- tswrdata::housing
plot(hh$approvals, type = "l", ylim = c(min(hh$activity), max(hh$approvals)))
points(hh$activity, type = "l", lty = 2)
```

It appears as though activity lags approvals by a quarter or so, though it is hard to see exactly. One way we can investigate the relatinship is through the _cross-correlation_ of the two variables. 

::: definition
The cross-correlation function at lag $k$ of two stationary time series is defined by 
\[
E[(x_{t + k} - \mu_x)(y_t - \mu_y)]
\]
:::

Here, as before, we are using stationary loosely to mean that the distribution of the time series does not depend on the time. 

This is similar to the autocorrelation function that we saw in the previous chapter. 

```{r}
ccf(hh$activity, hh$approvals)
```

Looking at the cross correlation plot, we see that there is a strong correlation between activity and approvals. It appears that the maximum correlation is at lag either 1 or 2 quarters, while there is significant correlation for several positive lags. This indicates that housing approvals are a lead variable for housing activity. However, it is not clear that the two time series that we started with were **stationary**. If the time series has a trend or seasonal component, then that will tend to dominate the cross correlation function. Sometimes that is what we want to do, but other times we are really interested in whether one variable is correlated with the **random** component of the other variable at some lag in order to improve predictions.

Therefore, let's decompose the two time series to remove trends and seasonal effects and re-do the cross correlation.

```{r}
activity <- ts(hh$activity, start = c(1996, 1), frequency = 4)
approvals <- ts(hh$approvals, start = c(1996, 1), frequency = 4)
activity_mod <- decompose(activity)
approvals_mod <- decompose(approvals)
ccf(activity_mod$random[-c(1, 2, 42, 43)], 
    approvals_mod$random[-c(1,2,42,43)], 
    main = "ccf of activity and approvals")
```

This plot shows that the **random** component of the decomposition of housing activity is correlated with housing approvals from 1 and/or 2 quarters prior. 

## Bass Model

The Bass model, according to Introductory Time Series with R by Cowpertwait and Metcalfe, is a "mathematical model which quantifies the theory of adoptin and diffusion of a new product by society." In this section, we are going to learn more about the Bass model and test it out on Blackberry sales figures. Let's go!

The Bass model is for the number of people $N_t$ who have bought the product under consideration at time $t$. A first thought would be to assume that the **change** in $N_t$ is proportional to the total number of people $m$ who are going to buy the product but haven't yet. (The change in $N_t$ is the number of people who buy the product in the time period $[t, t+1]$.) We write this as $p(m - N_t)$. These are the people who are initially interested in the product. 

Another group of people will also eventually buy the product. These are people who are influenced by seeing their friends or other people with the product. We model this as $(q N_t/m) (m - N_t)$. We can think of $q N_t/m$ as a time-varying proportion of people who will eventually by the product. As $N_t \to m$, we see that this value eventually becomes exactly $q$. In other words, this part starts by contributing nothing to the change in ownership but ends up contributing $q(m - N_t)$ each period.

In other words, we have 
\[
N_{t + 1} - N_t = p(m -N_t) + \bigl(q N_t/m \big)(m - N_t)
\]

In the Bass model, there are three parameters: $p$, $m$ and $q$.  We can solve for $N_t$ in the above equation, and get

\[
N_t = m\frac{1 - e^{-(p + q)t}}{1 + (q/p)e^{-(p + q)t}}.
\]

It is not very easy to see why this is true, so let's consider the a related continuous time problem. 

Assume that the time for a person to eventually buy the product has a probability density function given by $f(t)$, which depends on parameters $p$ and $q$. A plot of sales per time can then be obtained by multiplying $f(t)$ by $m$, the total number of people who will eventually buy the product. 

::: background

For a continuous random variable $X$ with pdf $f$, we define the _hazard function_ $h(t) = \frac{f(t)}{1 - F(t)}$. If $X$ denotes the time until a person buys a product, $h(t) \Delta t$ is the probability that a person who has not yet bought the item at time $t$ will buy it in the time interval $[t, t + \Delta t]$, at least for small values of $\Delta t$. If $X$ is the time until a person gets sick from a disease, then $h(t) \Delta t$ denotes the probability of getting sick "at time $t$" for people who have not yet gotten sick.

Let's look at some examples. If $X$ is uniform on the interval $[0, 1]$, this means that the time until a person buys the object is uniform between 0 and 1. Let's suppose that at time 1/2, the person has not yet bought the object. Let $Y$ be the time they buy the object given this new information. We have

\[
P(Y \le y) = \begin{cases}
0& y \le 1/2\\
P(X \le y|X \ge 1/2) & 1/2 \le y \le 1\\
1& y > 1
\end{cases}
\]

We compute $P(X \le y|X \ge 1/2) = P(1/2 \le X \le y)/P(X \ge 1/2) = \frac{y - 1/2}{1/2} = 2y - 1$. This **isn't** the hazard function. The hazard function gives the probability that the person will buy the object close to time $t$ given that they haven't yet bought it at time $t$. To compute that, we need to compute 

\begin{align*}
P(X \le t + \Delta t|X \ge t) &= \frac{P(t \le X \le t + \Delta t)}{1 - F(t)} \\
&= \Delta t \frac{F(t + \Delta t) - F(t)}{\Delta t}{1}{1 - F(t)} \\
&\approx \Delta t \frac{f(t)}{1 - F(t)},
\end{align*}
where in the last line we have used that when $\Delta t$ is small, $\frac{F(t + \Delta t) - F(t)}{\Delta t}$ is approximately the derivative of $F$ which is $f(t)$.


Let's compare that to the hazard function. Since $X$ is uniform, $f(t) = 1$ and $F(t) = t$, so 

\[
h(t) = \frac{1}{1 - t}
\]

For example, then, the probability that the person will buy the product between time $t = 0.9$ and time $t = 0.901$ is approximately $.001 \times \frac{1}{1 - .9} = .01$. It may be easier to think of this in terms of relative likelihoods. Since $h(.9) = 10$ and $h(0) = 1$, a person who has not yet bought at time $t = 0.9$ is 10 times more likely to buy the product in a short time span than the person at time 0. 
:::


The Bass model is most easily understood in terms of the hazard function. The Bass model says that there are coefficients $p$ and $q$ such that

\[
h(t) = p + qF(t),
\]

where again $F$ is the cumulative probability of having but one of the items. If we plug this in to the definition of hazard function, we get 

\[
p + qF(t) = \frac{f(t)}{1 - F(t)}.
\]

This is a differential equation that we can solve for $F(t)$ and obtain with some effort $F(t) = \frac{1 - e^{-(p + q)t}}{1 + (q/p)e^{-(p + q)t}}$. If we take the derivative and multiply by $m$, this gives us the sales per unit time. 

\[
S(t) = \frac{m(p + q)^2e^{-(p + q)t}}{p\bigl(1 + (q/p)e^{-(p + q)t}\bigr)^2}
\]

This is the equation that we will use to estimate $m$, $p$ and $q$ from data.

::: example
Let's consider the `blackberry` data set in the `tswrdata` package. This gives the quarter, year and sales data for handheld and table blackberry devices. 

```{r}
bb <- tswrdata::blackberry
plot(bb$handheld, type = "l")
```

The cumulative sales of blackberry devices is plotted below.

```{r}
plot(cumsum(bb$handheld), type = "l")
```

Close to 250 million were sold altogether. Our goal is to find $p$, $q$ and $m$ and see how well it matches the curve.  The way we will do it is with the `nls` function in R. This function finds the values of the parameters that minimize the least squares of the error estimates, similar to what `lm` does, but it doesn't need to be a linear model.

However, we do need to provide `nls` with some reasonable guesses for the parameters. Let's plot a few values of the Bass curve on top of our data and see if we can find one that is at least sort of close. After trying a few values, we see that $p = .001$, $q = .2$ and $m = 250$ works pretty well.

```{r}
bass <- function(p, q, m, x) {
  (m*(p + q)^2 * exp(-(p + q) * x))/(p * (1 + (q/p) * exp(-(p + q)* x))^2)
}
plot(bb$handheld, type = "l")
curve(bass(.1, .2, 200, x), add = T, col = 1)
curve(bass(.05, .2, 200, x), add = T, col = 2)
curve(bass(.001, .2, 250, x), add = T, col = 3)
```


```{r}
bb$time <- 1:nrow(bb)
mod <- nls(handheld ~ m * (p + q)^2 * exp(-(p + q) * time)/(p*(1 + (q/p)* exp(-(p + q)* time))^2),
    data = bb,
    start = list(p = .001, q = .2, m = 250))
a <- coefficients(mod)
plot(bb$handheld, type = "l")
curve(bass(a[1], a[2], a[3], x), add = T, col = 4)
```

This is not a bad fit!
:::

## Exponential Smoothing

In this section, we assume that we have a time series for which any trend and seasonal patterns have been removed. The mean of the time series may not be stationary (that is, it may depend on time), but it does so in a way that seems random and is not explained by trend or seasonal patterns. Our model is 

\[
x_t = \mu_t + z_t
\]

where $x_t$ is the observation, $\mu_t$ is the mean of the process at time $t$ and $z_t$ is a random error term that has mean 0 and standard deviation $\sigma$.

We estimate the mean at time $t$ by taking a weighted average of the estimate of the mean at time $t - 1$ and the value of the time series at time $t$. Naturally, when we only have one observation at time $t = 1$, we estimate the mean at time 1 $\hat \mu_1 = x_1$. For $t > 1$, we choose $0 < \alpha < 1$ and estimate $\hat \mu_t = (1 - \alpha) \hat \mu_{t - 1} + \alpha x_t.$

The value of $\alpha$ is a parameter that we need to decide on for our model. Values that are close to 0 indicate that the mean canges very little from time $t - 1$ to $t = 1$, and we would use this if we think that $\sigma$ is larger or  much larger than the change of mean over a time period of 1. When $\alpha$ is close to 1, this means that we are choosing the estimate of the current mean to be close to the current value of the time series. We use this when we expect the mean to change a lot in one time unit relative to $\sigma$. A common compromise value is $\alpha = .2$, or we can choose $\alpha$ that minimizes the one step ahead sum of squared error, see below.

Let's look at some plots that correspond to various $\alpha$ in the `motororg` data.

```{r}
mm <- tswrdata::motororg
alpha <- 0.9
a <- mm$complaints[1]
for(i in 2:48) a[i] <- alpha * mm$complaints[i] + (1 - alpha) * a[i - 1]

alpha <- 0.05
a2 <- mm$complaints[1]
for(i in 2:48) a2[i] <- alpha * mm$complaints[i] + (1 - alpha) * a2[i - 1]

plot(mm$complaints, type = "l")
points(a, type = "l", col = 2) #follows curve very closely when alpha = 0.9
points(a2, type = "l", col = 3) #smooths curve out when alpha = 0.05
```


:::theorem
If $x_t$ is a time series with infinite burn-in and we estimate the mean $\hat \mu_t$ using exponential smoothing, then
\[
\hat \mu_t = \sum_{i = 0}^\infty (1 - \alpha)^i \alpha x_{t - i}
\]
:::

In practice we do not have infinite burn-in, but when the burn-in is long relative to the size of $\alpha$, then we can estimate $\hat \mu_t = \sum_{i = 0}^t (1 - \alpha)^i \alpha x_{t  - i}$.

::: example
Consider the `motororg` data in the `tswrdata` package. This gives the number of complaints that a motor organization received from 1996 through 1999. We begin by plotting it.

```{r}
motor <- tswrdata::motororg
plot(motor$complaints, type = "l")
```

There doesn't appear to be any obvious seasonal component or trend to the data. Let's estimate the trend using exponential smoothing with $\alpha = 0.2$.

```{r}
x <- motor$complaints
mu <- x[1]
alpha <- .2
for(i in 2:48) mu[i] <- (1 - alpha) * mu[i - 1] + alpha * x[i]
plot(x)
points(mu, type = "l")
```
:::

If we want to "forecast" future values of the time series based on the current value $x_t$, we forecast as follows:

\[
\hat x_{t + k|t} = \hat \mu_t
\]

That is, our forecast for all future values is our best estimate for the current mean value. This is because we are assuming that there is no trend that we can understand, and any change in the mean value is random and unpredictable by us.

If we want to choose $\alpha$ dynamically based on the data, then we can do the following. For each $\alpha$, we compute the sequence $\hat \mu_t$, which is going to depend on $\alpha$. At each $t$, the **error** in the forecast $\hat x_{t + 1|t}$ is $\hat x_{t + 1} - \hat \mu_t$, so we can find the $\alpha$ that minimizes the sum of squared errors of the plus one forecasts. That is, we want to minimize

\[
\sum (\hat x_{t + 1} - \hat \mu_t)^2
\]

where $\hat \mu_t$ is computed as above, and depends on $\alpha$.

:::example
Compute the sum of squared errors for the `motororg` data when $\alpha 0.5$.

```{r}
alpha <- 0.5
a <- mm$complaints[1]
for(i in 2:48) a[i] <- alpha * mm$complaints[i] + (1 - alpha) * a[i - 1]
sum((a[1:47] - mm$complaints[2:48])^2)
```

:::

```{r}
ss1pe <- function(alpha) {
  mu <- x[1]
  for(i in 2:48) mu[i] <- (1 - alpha) * mu[i - 1] + alpha * x[i]
  sum((x[2:48] - mu[1:47])^2)
}
optimize(f = ss1pe, lower = 0, upper = 1)
ss1pe(.2)
```

The optimized $\alpha$ is 0.1429622, which gives a slightly different trend line in the plot, see the red line below as compared to the black line, which is $\alpha = 0.2$.

```{r echo=FALSE}
x <- motor$complaints
mu <- x[1]
alpha1 <- .2
alpha2 <- 0.1429622
mu2 <- x[1]
for(i in 2:48){
   mu[i] <- (1 - alpha) * mu[i - 1] + alpha * x[i]
   mu2[i] <- (1 - alpha2) * mu2[i - 1] + alpha2 * x[i]
}
plot(x)
points(mu, type = "l")
points(mu2, type = "l", col = 2)
```

The R function that implements exponential smoothing (and more) is `HoltWinters`. 

```{r}
mod <- HoltWinters(x = motor$complaints, beta = F, gamma = F)
mod$alpha #matches what we got above
fitted(mod)[,2] #these are the estimates for the mean
mod
```

The default is that `HoltWinters` estimates $\alpha$ from the data, as we did above. We can also supply a value for $\alpha$, The parameters $\beta$ and $\gamma$ will be explained in the next section. For exponential smoothing, we set them equal to `FALSE`.

```{r}
mod_point2 <- HoltWinters(x = motor$complaints, alpha = 0.2, beta = F, gamma = F)
fitted(mod_point2)[,2] #this match with the values we computed above
head(mu)
```

In order to forecast future values, we use the `predict` function.

```{r}
predict(mod, n.ahead = 2)
```

In this case, the future predictions are simply the estiamte for the last value of the level of the time series, which is not very interesting. However, we can also give error estimates.

```{r}
predict(mod, n.ahead = 2, prediction.interval = T, level = 0.95)
```

This says that, while our prediction for the number of complaints in the next two months are 17.7 in each month, our confidence decreases with time. For the first month, we are 95 percent confident it will be between $[3.51, 31.89]$ while for the second month we are 95 percent confident that it will be in the interval $[3.67, 32]$. 


## Holt Winters Method

In the previous section, we assumed that the mean changed unpredictably over time. In this section, we also assume that there is also a random **seasonality** component that is allowed to change over time as well as a random **slope** or **trend** component that is allowed to change over time. That is, we have three things that are allowed to change over time; the mean, the slope or trend, and the magnitude of the seasonality. 

::: alert
The **frequency** of the seasonality is not allowed to change over time in this model.
:::

Let's look at a plot of a typical data set that the Holt Winters method can be used with. The `auswine` data set contains monthly sales of various types of wine in Australia. We will look at sweet white wine.

```{r}
aa <- tswrdata::auswine
sweetwhite <- ts(aa$sweetw, frequency = 12, start = c(1980, 1))
plot(sweetwhite)
```

The sales seem to be periodic with a trend, but the magnitude of the seasonal component appears to change with time. We could try a multiplicative seasonal model using `decompose`, and I encourage you to do so in order to compare with what we decide to do below. We do things a bit the other way around in this section, and begin with the R code of our new model. We might want to choose a multiplicative seasonal model because it seems that the seasonal variation is closer to being multiplicative than additive, but we will do an additive one.

```{r}
mod_hw <- HoltWinters(sweetwhite, seasonal = "additive")
plot(mod_hw)
acf(residuals(mod_hw))
```

There still seems to be some periodicity in the residuals of the model, perhaps we should change the frequency to 6?

```{r}
sweetwhite_6 <- ts(aa$sweetw, frequency = 6, start = c(1980, 1), end = c(1997, 6)) 
mod_hw_6 <- HoltWinters(sweetwhite_6, seasonal = "mult")
plot(mod_hw_6)
acf(resid(mod_hw_6))
```

That seems even worse, so we'll stick with the first one. We could also have used the functin `findfrequency` in the `forecast` package to estimate the frequency of the data. We get 12 there, as well.

```{r}
forecast::findfrequency(aa$sweetw)
```

Let's see what the Holt-Winters model outputs.

```{r}
mod_hw
```


We note hear that the Holt-Winters exponential smoothing procedure produced estimates for three parameters; for $\alpha$, $\beta$ and $\gamma$, and provided values of 14 coefficients. The model is:

\begin{align*}
a_t &= \alpha(x_t - s_{t - p}) + (1 - \alpha) (a_{t - 1} + b_{t - 1}) \\
b_t &= \beta (a_t - a_{t- 1}) + (1 - \beta) b_{t - 1}\\
s_t &= \gamma(x_t - a_t) + (1 - \gamma) s_{t - p}
\end{align*}

Here, $a_t$ is the **level** of the time series, $b_t$ is the **slope** or **trend** of the time series, and $s_t$ is the seasonal effect of the time series all at time $t$. In this case, the mean of the time seires is $a_t + s_t$, the level of the time series plus the seasonal effect at time $t$. The parameters $\alpha$, $\beta$ and $\gamma$ are **smoothing parameters**, and $p$ is the period of the time series (which we call the **frequency** above). Let's think about each of these equations.

First, $x_t$ is the thing that we know; the data. So when estimating $a_t$, we can only use $x_t$ and things from the past. Since $E[x_t] = a_t + s_t$, our estimate for $x_t$ might be $a_t + s_t$. However, we can't use $s_t$, so we substitute $s_{t - p}$, and solve for $a_t$. That's where the $x_t - s_{t - p}$ comes from. Alternatively, we could estimate the level of the time series as the level of the time series at the previous point in time $a_t \approx a_{t - 1}$. But, since we have **slope** estimates, we estimate $a_t \approx a_{t - 1} + b_{t - 1}$. The final estimate for $a_t$ is a weighted average of these two separate estimates, where the weight is a parameter. Whew!

All right, then. What about $b_t$?  This is the slope at time $t$. Now, we are allowed to use $x_t$ **and** our estimate for $a_t$ that we computed above, but not $s_t$ since we do not have an estimate for it. This one is easier. The quantity $a_t - a_{t - 1}$ is the estimated change in level of the time series between $t$ and $t - 1$, which is the slope. On the other hand, $b_{t - 1}$ was our esimate for the change in level of time series at time $t - 1$, so we take a weighted average of these two values.

Finally, what about $s_t$? This is the seasonal component. Recall that $E[x_t] = a_t + s_t$, so one estimate for $s_t$ is $x_t - a_t$. The other estimate is $s_{t - p}$, which is our previous estimate for the seasonal component at this place in the period. We take a weighted average of those two values to get our estimate for $s_t$.

The **coefficients** provided by `HoltWinters` are the last values of the level and trend, and the last `frequency` many values of the seasonal component. Since the frequency is 12, it provides 12 values.

How do we find good values for the parameters $\alpha$, $\beta$ and $\gamma$? Well, once again, we minimize the SS1PE. However, in this case, we need not only to esimate the smoothing parameters, but also $a_p$, $b_p$, and $s_1, \ldots, s_p$. Oh, dear. I am not sure I want to take a stab at this. Should I? OK. I'll do it, but only because you asked me to.

::: {.example #exm:sweetwinehm}
To estimate the seasonal values, we look at the first two periods of the time series and use `decompose`. 

```{r}
mod_decompose <- decompose(ts(sweetwhite[1:24], frequency = 12))
```

This means that we will estimate the seasonal effect for January-December of the first year as follows:

```{r}
mod_decompose$figure
```

To estimate the values of level and slope, we do a linear regression on the trend over the first two periods. The starting slope is the estimated slope, and while the intercept should seem to correspond to June of the first year, we will use it as our estimate for January in the second year. TBH, I am not sure why `HoltWinters` does this; it would seem that a better estimate would be the predicted value of the line of best fit at $t = 7$ (see Exercise \@ref(ex:whyintercept)).

```{r}
#this function assumes time series is stored in variable called x
#frequency is a constant, thank goodness
#will probably crash if frequency is less than length of data

s <- mod_decompose$figure #estimate for starting seasonality
trend <- na.omit(mod_decompose$trend) #note that this is JULY-JUNE; missing JAN-JUN of first year!!
tt <- 1:12
mod_lm <- lm(trend ~ tt)

#intercept of model; initial estimate for mean level in JUNE, but we use for estimate for January of year 2
a <- coefficients(mod_lm)[1] 

b <- coefficients(mod_lm)[2] #initial estimate for slope

ss1pe <- function(pars, frequency, debug = FALSE) {
  if(debug) {browser()}
  alpha <- pars[1]
  beta <- pars[2]
  gamma <- pars[3]
  at <- a
  bt <- b
  st <- s
  at <- rep(a, frequency) #repeating 12 times to make length same as st
  bt <- rep(b, frequency) #repeating 12 times to make length same as st
  st <- s
  for(i in (frequency + 1):length(x)) {
    at[i] <- alpha * (x[i] - st[i - frequency]) + (1 - alpha) * (at[i - 1] + bt[i - 1])
    bt[i] <- beta * (at[i] - at[i - 1]) + (1 - beta) * bt[i - 1]
    st[i] <- gamma * (x[i] - at[i]) + (1 - gamma) * st[i - frequency]
  }
  at <- at[-(1:(frequency - 1))] #get rid of the garbage data added above
  bt <- bt[(-(1:frequency - 1))] #same
  st <- st[-((length(st) - 11):length(st))] #st has garbage at the end
  x <- x[-(1:(frequency))] #the initial year of data is not estimated; estimates start in January
  sum((x - (at[-length(at)] + bt[-length(at)] + st))^2) #sse; note that at + bt + st is the estimate for xt
}
x <- as.vector(sweetwhite)
optim(par = c(.2, .2, .2), fn = ss1pe, frequency = 12, method = "L-BFGS-B", lower = rep(0, 3), upper = rep(1, 3))
mod_hw$SSE
```

We see that we get the same values for the smoothing parameters as Holt-Winters, as well as the same values of the SSE. The coefficients given by Holt-Winters are the values of `a`, `b` and `s` that we estimated in the previous R chunk.
:::

## Exercises

::: {.exercise #ex:hazardfunction}
Suppose the time to purchase $X$ is an exponential random variable with rate $\lambda = 2$. 

a. Find the hazard function for the time of purchase.
b. Exponential random variables have the "memoryless property." Explain how your answer above can be interpreted as a memoryless property.
:::

::: {.solution #sol:hazardfunction}

The pdf of $X$ is $f(x) = 2 e^{-2x}$, so the cdf of $X$ is $F(x) = 1 - e^{-2x}$. This means that the hazard function is

\[
h(t) = \frac{f(t)}{{1 - F(t)}} = \frac {2e^{-2x}}{e^{-2x}} = 2
\]

This is the memoryless property because it doesn't matter how long a person has gone without buying the product, the likelihood of them buying in the next little bit remains the same. It doesn't "remember" that it has been a long time that they haven't bought the product.
:::

::: {.exercise #ex:blackberrybass}
Consider the `blackberry` data in the `tswrdata` package. Estimate the values of $p, q$ and $m$ in the Bass model for number of handheld devices sold using only the first 20 data points and plot the resulting Bass curve on top of the data. 

a. Comment on how well the model fits all of the data.
b. How many blackberries does this model estimate will be sold overall?
:::

::: {.solution #sol:blackberrybass}
```{r}
bass <- function(p, q, m, x) {
  (m*(p + q)^2 * exp(-(p + q) * x))/(p * (1 + (q/p) * exp(-(p + q)* x))^2)
}
bb <- tswrdata::blackberry
bb$time <- 1:nrow(bb)
bb_mod <- nls(handheld ~ bass(p, q, m, time), data = bb, start = list(p = 0.001, q = .3, m = 400), subset = 1:20)
bb_mod <- nls(handheld ~ bass(p, q, m, time), data = bb[1:20,], start = list(p = 0.001, q = .3, m = 400), subset = 1:20)
a <- coefficients(bb_mod)
plot(bb$handheld)
curve(bass(a[1], a[2], a[3], x), add = T, col = 2)
```

This overestimates the total sales, but not by too much. The total sales are estimated to be `r round(a[3])` million units.
:::

::: {.exercise #ex:ipodbass}
Consider the `ipod` data in the `tswrdata` package. Estimate the values of $p$, $q$ and 4m$ in the Bass model for the number of ipods sold **using only the first 20 data points**. Plot the resulting Bass curve on top of **all** of the data and comment on the fit.
:::

::: {.solution #sol:ipodbass}

```{r}
bass <- function(p, q, m, x) {
  (m*(p + q)^2 * exp(-(p + q) * x))/(p * (1 + (q/p) * exp(-(p + q)* x))^2)
}
ipod <- tswrdata::ipod
ipod$time <- 1:nrow(ipod)
ipod_mod <- nls(i_pod ~ bass(p, q, m, time), data = ipod, start = list(p = 0.01, q = .1, m = 100), subset = 1:20)
a <- coefficients(ipod_mod)
plot(ipod$i_pod)
curve(bass(a[1], a[2], a[3], x), add = T, col = 2)
```

While the fit is very good up through about index 20, it becomes poor thereafter. It underestimates the total number of units sold as well. This model did not do a good job of predicting the sales.

:::

::: {.exercise #ex:ss1peexplore}
Consider the `motororg` data in the `tswrdata` package. What is the SS1PE if you predict the number of complaints in the following month to be equal to the number of complaints in the current month?
:::

::: {.solution #sol:ss1peexplore}
```{r}
mm <- tswrdata::motororg
sum((mm$complaints[1:47] - mm$complaints[2:48])^2)
```

The SS1PE is 3540, which is larger than the minimum value of 2502 that we found for $\alpha = .143$.
:::

::: {.exercise #ex:ss1peex}
Consider the adjusted price of Apple stock, which can be loaded into the variable `AAPL` using `quantmod::getSymbols("AAPL")`. Restrict to calendar year 2019 using `AAPL[str_detect(index(AAPL), "2019")]` (for this, you will need the `zoo` package).

a. Modify the function `ss1pe` given in the notes above to find the optimal value of $\alpha$ in an exponential smooothing model for the adjusted price of Apple in 2019. You will probably want to convert `AAPL$AAPL.Adjusted` to a numeric vector by using `as.vector(AAPL$AAPL.Adjusted)`.
b. Give an explanation for why $\alpha$ is so large. Include a discussion of the **trend** of the time series.
c. Plot the exponentially smoothed time series of adjusted price for the optimal $\alpha$ on the same plot as the adjusted price.
d. Plot the exponentially smoothed time series of adjusted price for $\alpha = 0.2$ on the same plot as the adjusted price.
e. Is this an appropriate model for this data set?
:::

::: {.solution #sol:ss1peex}

```{r}
library(quantmod)
library(zoo)
library(stringr)
quantmod::getSymbols("AAPL")
aa <- AAPL[str_detect(index(AAPL), "2019")]
plot(aa$AAPL.Adjusted)
```

```{r}
ss1pe <- function(alpha) {
  mu <- as.vector(aa$AAPL.Adjusted[1])
  for(i in 2:nrow(aa)) mu[i] <- (1 - alpha) * mu[i - 1] + alpha * aa$AAPL.Adjusted[i]
  sum((aa$AAPL.Adjusted[2:nrow(aa)] - mu[1:(nrow(aa) - 1)])^2)
}
optimize(f = ss1pe, lower = 0, upper = 1)
HoltWinters(as.vector(aa$AAPL.Adjusted), beta = F, gamma = F) #same answer for alpha
```

It is large because the trend is locally larger than the random component of the plot. The data looks to be composed of a series of more or less straight lines.

```{r}
plot(as.vector(aa$AAPL.Adjusted))
alpha <- 0.9668728
mu <- as.vector(aa$AAPL.Adjusted[1])
for(i in 2:nrow(aa)) mu[i] <- (1 - alpha) * mu[i - 1] + alpha * aa$AAPL.Adjusted[i]
points(mu, type = "l", col = 2)
```

Note how for $\alpha = 0.2$, the estimate seems to be trailing the trend - it is either below the plot when the stock is increasing or above it when it is decreasing.

```{r}
plot(as.vector(aa$AAPL.Adjusted))
alpha <- 0.2
mu <- as.vector(aa$AAPL.Adjusted[1])
for(i in 2:nrow(aa)) mu[i] <- (1 - alpha) * mu[i - 1] + alpha * aa$AAPL.Adjusted[i]
points(mu, type = "l", col = 3, lty = 2)
```

:::


::: {.exercise #ex:whyintercept}
In this exercise, we investigate the choice of picking the intercept of the linear model of trend on time to be the initial value of the level at time $t = p + 1$. Modify the code in Example \@ref(exm:sweetwinehm) so that the initial value for the level in January is not the intercept of the linear model through the trend of the decomposed first two periods, but rather the predicted value for January of the linear model through the trend of the decomposed first two periods.
:::


::: {.solution #hide:whyintercept}

```{r}
sweetwhite <- tswrdata::auswine$sweetw
mod_decompose <- decompose(ts(sweetwhite[1:24], frequency = 12))
```

```{r}
sweetwhite <- tswrdata::auswine$sweetw
mod_decompose <- decompose(ts(sweetwhite[1:24], frequency = 12))
s <- mod_decompose$figure #estimate for starting seasonality
trend <- na.omit(mod_decompose$trend) #note that this is JULY-JUNE; missing JAN-JUN of first year!!
tt <- 1:12
mod_lm <- lm(trend ~ tt)
a <- coefficients(mod_lm)[1] 
b <- coefficients(mod_lm)[2] #initial estimate for slope
a <- a + 7 * b #this is the prediction for January of year 2, and the only change in the code
ss1pe <- function(pars, frequency, debug = FALSE) {
  if(debug) {browser()}
  alpha <- pars[1]
  beta <- pars[2]
  gamma <- pars[3]
  at <- a
  bt <- b
  st <- s
  at <- rep(a, frequency) #repeating 12 times to make length same as st
  bt <- rep(b, frequency) #repeating 12 times to make length same as st
  st <- s
  for(i in (frequency + 1):length(x)) {
    at[i] <- alpha * (x[i] - st[i - frequency]) + (1 - alpha) * (at[i - 1] + bt[i - 1])
    bt[i] <- beta * (at[i] - at[i - 1]) + (1 - beta) * bt[i - 1]
    st[i] <- gamma * (x[i] - at[i]) + (1 - gamma) * st[i - frequency]
  }
  at <- at[-(1:(frequency - 1))] #get rid of the garbage data added above
  bt <- bt[(-(1:frequency - 1))] #same
  st <- st[-((length(st) - 11):length(st))] #st has garbage at the end
  x <- x[-(1:frequency)] #the initial year of data is not estimated; estimates start in January
  sum((x - (at[-length(at)] + bt[-length(at)] + st))^2) #sse; note that at + bt + st is the estimate for xt
}
x <- as.vector(sweetwhite)
optim(par = c(.2, .2, .2), fn = ss1pe, frequency = 12, method = "L-BFGS-B", lower = rep(0, 3), upper = rep(1, 3))
HoltWinters(ts(sweetwhite, frequency = 12))$SSE
```

Note that the SSE **decreases** from 548748.9 to 546607.2. This is kind of to be expected, since we know that `a + 7 * b` is a better estimate for the level in January of year 2 than `a` is. Instead of re-writing the entire algorithm, we could also have specified the start values for level, slope and seasonal values inside of `HoltWinters`. 

```{r}
#recall a <- a + 7 * b above
mod_hw_2 <- HoltWinters(ts(sweetwhite, frequency = 12), l.start = a, b.start = b, s.start = s)
mod_hw_2$SSE #matches what we obtained
```

```{r echo = FALSE, eval = FALSE}
length <- 200
frequency <- 12
b <- 1
b_change <- cumsum(runif(200, rep(c(-.001, -.01), each = 100), rep(c(.001, .01), each = 100)))
b_change
for(i in 2:200) b[i] <- b[i-1] + b_change[i]
s <- runif(1, .1, 2) * sin(1:12/12 * 2 * pi) + 
  runif(1, .1, 1) * cos(1:12/12 * 2 * pi) + 
  runif(1, -.5, .5) * sin(1:12/6 * 2 * pi)
for(i in 13:200) s[i] <- s[i - 12] + runif(1, -.05, .05)
a <- 1
for(i in 2:200) a[i] <- a[i - 1] + b[i - 1] + if(runif(1) < .99) {runif(1, -.05, .05)} else {sample(c(-1, 1), 1) * rpois(1, 50)}
x <- a + 5 * b + 10 * sd(b) * s + rnorm(200, 0, 1/3 * sd(a + b + s))
plot(x, type = "l")
hwmod <- HoltWinters(ts(x, frequency = 12))
hwmod$SSE
hwmod
mod_decompose <- decompose(ts(x[1:24], frequency = 12))
s <- mod_decompose$figure #estimate for starting seasonality
trend <- na.omit(mod_decompose$trend) #note that this is JULY-JUNE; missing JAN-JUN of first year!!
tt <- 1:12
mod_lm <- lm(trend ~ tt)
#intercept of model; initial estimate for mean level in JUNE, but we use for estimate for January of year 2
a <- coefficients(mod_lm)[1] 
b <- coefficients(mod_lm)[2] #initial estimate for slope
a <- a + 7 * b
optim(par = c(.2, .2, .2), fn = ss1pe, frequency = 12, method = "L-BFGS-B", lower = rep(0, 3), upper = rep(1, 3))
```

:::

::: {.exercise #ex:vaccineholt}
Consider the `vaccine` data in the `tswrdata` package. 

a. Create a Holt Winters model with level, trend and seasonal parameters and report the values of the smoothing parameters.
b. Predict the number of vaccines that are going to be given in the next 7 days (from the end of the data set), together with 95 percent prediction intervals. Plot.
:::

::: {.solution #sol:vaccineholt}
```{r}
vv <- tswrdata::vaccines
library(tidyverse)
vv <- mutate(vv, date = lubridate::ymd(date))
vv <- arrange(vv, date)
vv_mod <- HoltWinters(ts(vv$doses, frequency = 7))
plot(vv_mod)
vv_mod$SSE
forecast <- predict(vv_mod, n.ahead = 7, prediction.interval = .95)
plot(vv_mod, forecast)
```

:::

::: {.exercise #ex:stltemps}
Consider the `stltemp` data set from the `tswrdata` package. Restrict to data from Lambert airport on or later than January 1, 1970 (You may need to remove the last data point, which has NA for `tmax`.)

a. Remove all leapdays, in order to keep the frequency consistent at 365 days.
a. Create a Holt - Winters model for temperature. 
b. Forecast the high temperatures for the next 30 days after the end of the data, with 95 percent prediction intervals, and plot.
c. Comment on whether the 95 percent prediction interval for February 13, 2021 is reasonable, too big, or too small, by comparing the values to the all-time record low and high for `tmax` in February at Lambert.
:::


::: {.solution #sol:stltemps}
```{r}
ss <- tswrdata::stltemp
library(tidyverse)
library(lubridate)
ss$date <- ymd(ss$date)
ss <- filter(ss, str_detect(name, "LAMBERT"), !str_detect(date, "02-29"))
ss <- filter(ss, date >= "1970-01-01")
mod_ss <- HoltWinters(ts(ss$tmax[!is.na(ss$tmax)], frequency = 365, start = c(2010, 1)))
broom::tidy(predict(mod_ss, n.ahead = 30, prediction.interval = T, level = 0.9)) %>% 
  mutate(date = rep(seq(ymd("2021-01-19"), length.out = 30, by = "day"), each = 3)) %>% 
  filter(date == "2021-02-13")
```

The prediction interval for Feb 13 seems too wide at first glance.

```{r}
ss <- tswrdata::stltemp
ss %>% 
  filter(month(date) == 2) %>% 
  summarize(min = min(tmax, na.rm = T),
            max = max(tmax, na.rm = T))
```

The 95 percent prediction interval for Feb 13 contains all past `tmax` values for that date, which means that it is almost certainly too wide of an interval to be exactly a 95 percent prediction interval.
:::

::: {.exercise #ex:amtrakridership}
Consider the `amtrak` data set in the `tswrdata` package. 

a. Plot the data and determine whether an exponential smoothing  model with level or a Holt-Winters model with level, trend and seasonality is more appropriate. 
b. Model the data according to your answer in b.
c. Plot the data together with the model and forecast Amtrak ridership for the next 12 months.
:::

::: {.solution #sol:amtrakridership}
```{r}
aa <- tswrdata::amtrak
library(tidyverse)
ggplot(aa, aes(x = month, y = num_passengers)) + 
  geom_line()
```

There seems to be a trend and seasonality, so we need to use Holt - Winters.

```{r}
forecast::findfrequency(aa$num_passengers)
aap <- ts(aa$num_passengers, frequency = 12, start = c(1997, 1))
mod_aa <- HoltWinters(aap)
plot(mod_aa) #looks pretty good
forecast <- predict(mod_aa, n.ahead = 12, prediction.interval = T, level = .95)
plot(mod_aa, forecast)
```

:::

::: {.exercise #ex:amtrakridership2}
Consider the `amtrak` data set in the `tswrdata` package. 

a. We can remove the trend and seasonal components of the data by **differencing**. If $x_t$ is the time series, let $y_t = x_{t + 1} - x_{t}$ and $z_t = y_{t + 12} - y_{t}$. This can be accomplished in R via `y = diff(x)` and `z = diff(y, lag = 12)`. 
b. Perform the differencing described above and plot the twice-differenced data. Can you see any trend or seasonality component?
c. Use exponential smoothing with level only to model the twice differenced data and plot.
d. We can change the default starting value of exponential smoothing by specifying `l.start`. Note that the original value seems very low and not typical of the values of the data set. Re-do part c, specifying `l.start = 0` and comment.
e. Choose `l.start` to be the mean of the first seven data points and re-do part c.
:::

::: {.solution #sol:amtrakrtidership2}
```{r}
aa <- tswrdata::amtrak$num_passengers
y <- diff(aa)
z <- diff(y, lag = 12)
plot(z, type = "l")
forecast::findfrequency(z)
```

I do not see any seasonality or trend, but `findfrequency` thinks there is a frequency of 3.

We continue with exponential smoothing.

```{r}
mod_z <- HoltWinters(z, beta = FALSE, gamma = FALSE)
plot(mod_z)
```


```{r}
mod_z <- HoltWinters(z, beta = FALSE, gamma = FALSE, l.start = 0)
plot(mod_z)
mod_z
```

Note that $\alpha$ is essentially zero. This means that we are not changing it very much at all when getting new data, and just keeping the value at zero throughout the time series. 

```{r}
lstart <- mean(z[1:7])
mod_z <- HoltWinters(z, beta = FALSE, gamma = FALSE, l.start = lstart)
plot(mod_z)
mod_z
```

:::




# Stochastic Models

In the previous chapters, we saw how to model two types of data. We used `decompose` to model data that had a constant seasonal component, a trend and error. We used `HoltWinters` to model data that has a level and/or a trend and/or a seasonal component, all of which could vary with time. 

A big question we have is: how do we know whether the model we have so far is sufficient to describe the data? One way of determining is to look at the **residuals**. The residuals are the differences between the predicted values at each time and the  observed values. IF there is no longer any dependence on time, then we have a good model for the data. If we can still see (via `acf`, for example) some dependence on time, then we need to do some more work. 

## White Noise Model

Since our primary interest is in modeling residuals, let's start with a model that describes **ideal** residuals. This model is what we are hoping to get in the end. It is called a **white noise** model.

::: definition
Let $w_t$ be iid random variables with mean 0 and standard deviation $\sigma$. Then $w_t$ is white noise. If, in addition, $w_t$ is normally distributed, then it is Gaussian white noise.
:::

By definition, the autocorrelation of white noise will have the value 1 at lag 0, and zero at every other lag. Of couse, when viewing the correlogram of data, we will not get **exactly** zero at each of the other lags. It can take some practice to determine what is OK and what isn't  I recommend running the below code several times, with different values of `n`, to get a feel for the correlogram for Gaussian white noise and white noise which is uniformly distributed.

```{r}
acf(rnorm(n = 50))
acf(runif(n = 100, min = -1, max = 1))
```

If we want to model white noise, then the only parameter we need to estimate is $\sigma$. We can use the R function `sd` to get an estimate for $\sigma$. 

::: example
Consider the `amtrak` data set in the `tswrdata` package. This data gives the number of Amtrak passengers per month. You were asked to plot and examine this data set in Exercise \@ref(ex:amtrakridership).

```{r}
aa <- tswrdata::amtrak
mod_aa <- HoltWinters(ts(aa$num_passengers, frequency = 12))
acf(residuals(mod_aa))
plot(residuals(mod_aa))
```

In this case, we see that there seems to be no correlation between residuals at various times. In order to fully specify the white noise model of the residuals, we simply need to estimate the variance or standard deviation.\

```{r}
sd(residuals(mod_aa))
```

The residuals appear to be white noise with $\sigma \approx 55.8$.
:::

## Random walks and random walks with drift

We present two simple models which are built from white noise processes in this section, and show an example of a time series that seems to be well-modeled by one of them.

::: definition
Let $w_t$ be a white noise process with standard deviation $\sigma$. The random walk process generated by $w_t$ is defined by $x_1 = w_1$ and $x_t = x_{t - 1} + w_t$. The random walk process with drift $\delta$ is defined by $x_1 = w_1$ and $x_t = x_{t - 1} + \delta + w_t$.
:::

We note here that a random walk is an AR(1) model with $\alpha = 1$. As such, we have already computed its variance as $V(x_t) = \sigma^2\bigl(\sum_{i = 0}^{t - 1} \alpha^{2i}\bigr) = t\sigma^2$. We have to re-compute the acf because in the case of an AR(1) model, we assumed infinite burn-in. This doesn't make sense for random walks, because $\alpha = 1$ and the infinite series obtained is not finite. Since this is **not** a stationary series, we compute

\begin{align*}
\rho_k(t) &=  \frac{{\text {Cov}}(x_t, x_{t + k}}{\sqrt{{\text{Var}}(x_t){\text{Var}}(x_{t + k})}}\\
&= \frac{t\sigma^2}{\sqrt{t\sigma^2(t + k)\sigma^2}} \\
&= \frac 1{\sqrt{1 + k/t}}
\end{align*}

To finish this, we need to understand why ${\text {Cov}}(x_t, x_{t + k}) = t\sigma^2.$ Let's write it out:

\begin{align*}
{\text {Cov}}(x_t, x_{t + k}) &= {\text {Cov}}\bigl(\sum_{i = 1}^t w_i, \sum_{j = 1}^{t + k} w_i\bigr)\\
&= \sum_{i = j}  {\text {Cov}}(w_i, w_j) = t\sigma^2
\end{align*}

Let's see what this looks like. We note that `acf` assumes that the autocorrelation is the same at each value of $t$, which is not true in this case. However, we still want to know what the correlogram from a random walk looks like.

```{r}
w <- rnorm(200)
x <- w[1]
for(i in 2:length(w)) x[i] <- x[i - 1] + w[i]
acf(x)
```

We see we get the typical acf that is associated with an AR(1) model, even though in this case the model is not stationary. Let's also look at some plots.

```{r}
plot(x, type = "l")
```

This is a typical plot for a random walk. It might be surprising that the values vary so much, but remember that the variance of the series at time $t$ is $t\sigma^2$, so we expect the values to be between $\pm 2\sqrt{t}$ when $\sigma = 1$, so for example when $t = 100$, we expect the value of $x$ to be between -20 and 20.

Next, let's simulate some data for random walk with drift. We assume $\sigma = 1$ and plot data for various values of $\delta$.

```{r}
w <- rnorm(200)
x1 <- w[1]
delta <- .1
for(i in 2:200) x1[i] <- x1[i - 1] + delta + w[i]
x2 <- w[1]
delta <- .2
for(i in 2:200) x2[i] <- x2[i - 1] + delta + w[i]
x3 <- w[1]
delta <- .5
for(i in 2:200) x3[i] <- x3[i - 1] + delta + w[i]

plot(x3, type = "l")
points(x2, type = "l", col = 2)
points(x1, type = "l", col = 3)
```

Notice that if $x_t$ is a random walk or a random walk with drift, then the **differences** $x_t - x_{t - 1}$ will be white noise, possibly **shifted** by $\delta$.

```{r}
acf(diff(x3))
acf(diff(x2)) #these are exactly the same because the drift cancels!
```

But, 

```{r}
hist(diff(x3))
mean(diff(x3)) #true value of delta is 0.5
hist(diff(x1))
mean(diff(x1)) #true value of delta is 0.1
```

::: example
Simulate random walk with drift $\delta = 0.25$ 10,000 times and see whether the estimate of $\delta$ from the mean of the differences is biased or unbiased.

```{r}
sim_data <- replicate(10000, {
  w <- rnorm(200)
  x <- w[1]
  for(i in 2:200) x[i] <- x[i - 1] + 0.25 + w[i]
  mean(diff(x))
})
mean(sim_data)
```

This is very close to the true value of 0.25, and if you run it multiple times then you will see that it is not consistently over or underestimating the true value of the drift.
:::

::: example
Consider the exchange rate data in `samarket` in the `tswrdata` package. Let's plot the ZAR-USD exchange rate.

```{r}
zz <- tswrdata::samarket$zar_usd
zz <- zz[!is.na(zz)]
plot(zz, type = "l")
```

That might be a random walk; it is hard to tell. If there is any drift, it seems to be small because the series has periods where it decreases and periods where it increases.

```{r}
acf(diff(zz))
```

The auto-correlation is negative at lags 1 and 2, so a random walk model is **not** sufficient to describe this data set. 
:::

::: example
Consider the `eggs` data set in the `fma` package. This gives the price of eggs in the US in constant (adjusted for inflation) dollars. 

Let's plot the data.

```{r}
plot(fma::eggs)
```

This appears that it could be a random walk or random walk with drift. Let's look at the correlogram of the differences.

```{r}
acf(diff(fma::eggs))
```

There is a significant lag at $k = 14$, but the random walk model (possibly with drift) seems reasonable. How do we know whether there is drift? We need to look at the differences and see whether they are significantly different from 0.

```{r}
t.test(diff(fma::eggs))
```

We see that we do **not** reject the null hypothesis that the drift is zero, so we conclude that a random walk (without drift) models this data sufficiently.
:::

::: example
In this example, construct a random walk with drift 0.2 of length 300 and conduct a `t.test` on the differences to determine whether we can detect the drift.

```{r}
w <- rnorm(300)
x <- w[1]
for(i in 2:300) x[i] <- x[i - 1] + 0.2 + w[i]
t.test(diff(x))
```
:::

## General Autoregressive Processes

An AR($p$) process is a generalization of the AR(1) process that we considered in Section \@ref(introautoregressive). Let $z_t$ be white noise. If for $t > p$ $x_t = \alpha_1 x_{t - 1} + \cdots + \alpha_p x_{t - p} + z_t$, then $x_t$ is generated by an autoregressive preocess of order $p$. We say that $x_t$ is an AR($p$) process.

In the definition of an AR($p$) process, we do not require any specific properties of the first $p$ values of the time series. Let's see how to simulat it, and what plots will look like. We consider the case that $p = 2$.

```{r}
alpha_1 <- .263
alpha_2 <- .75
x <- 1
x[2] <- 2
z <- rnorm(300, 0, 1)
for(i in 3:300) x[i] <- alpha_1 * x[i - 1] + alpha_2 * x[i - 2] + z[i]
plot(x, type = "l")
acf(x)
```

```{r eval = FALSE, echo = FALSE}
# this is how the example was found

fn <- function(x) {
  alpha_2 * x^2 + alpha_1 * x - 1
}
alpha_2 <- .85
abs(polyroot(z = c(1, alpha_1, alpha_2)))
ee <- lapply(seq(-1, 1, length.out = 20), function(alpha_1) {
  lapply(seq(-1, 1, length.out = 25), function(alpha_2) {
    list(alpha1 = alpha_1, 
         alpha2 = alpha_2, 
         roots = min(abs(polyroot(z = c(-1, alpha_1, alpha_2)))))
  })
})
ee <- bind_rows(ee)
mutate(ee, sum = abs(alpha1) + abs(alpha2)) %>% arrange(abs(sum - 1)) %>% 
  print(n = 30)
```

```{r}
alpha_1 <- .263
alpha_2 <- -.75
x <- 1
x[2] <- 2
z <- rnorm(300, 0, 1)
for(i in 3:300) x[i] <- alpha_1 * x[i - 1] + alpha_2 * x[i - 2] + z[i]
plot(x, type = "l")
acf(x)
```

See the difference between the two time series. The second one seems to be **stationary** in the sense that the characterstics of the time series don't change over time. The mean value and the variance seem to be about the same at time 300 as at time 150. Note, however, that these charactersitcs are not shared by the first few data points; $x_1 = 1$ has variance 0, for example. However, if we assume an infinite burn-in period, then the time series is stationary, and it is also approximately stationary for large, finite burn-in periods.

Contrast this with the first time series. The ccharactersitics (variance, for example) **do** seem to depend on time, and the time series is not stationary. Re-run the code above a few times to convince yourself that the variance at time 200 is larger than the expected value at 20, for example.

::: tryit
Consider the other two choices of signs on .263 and .75, namely $\alpha_1 = -.263$ and $\alpha_2 = \pm .75$. Guess whether the corresponding time series' will be stationary or not, and then plot a few simulations to check your work.
:::

Looking at the four examples above, it is perhaps not obvious what the pattern is for determining whether an AR($p$) process is stationary based on its coefficients. That is the purpose of the next theorem.

::: theorem
Consider an AR($p$) process with coefficients $\alpha_1, \ldots, \alpha_p$, so that 
\[
x_t = \sum_{i = 1}^p \alpha_i x_{t - i} + \epsilon_t
\]
Then $(x_t)$ is stationary if and only if **all** of the roots of the polynomial
\[
-1 + \sum_{i = 1}^p \alpha_i x^i
\]
are greater than 1 in modulus.
:::

For practical purposes, we can use the function `polyroot` to find the roots of a polynomial, and combine with `abs` which computes the modulus of a complex number.

::: example
Show that the examples considered graphically above, $\alpha_1 = .263$ and $\alpha_2 = \pm .75$ are characterized correctly by this theorem.

```{r}
alpha_1 <- .263
alpha_2 <- -.75
abs(polyroot(z = c(-1, alpha_1, alpha_2)))
```
We see that this process **is** stationary since one of the roots is 0.992 in modulus.

```{r}
alpha_1 <- .263
alpha_2 <- .75
abs(polyroot(z = c(-1, alpha_1, alpha_2)))
```
This process is **not** stationary, since all of the roots have modulus greater than 1.
:::

::: example
How can we find $p$ when given data that we suspect comes from an AR($p$) process? 

For example, suppose that we have data that comes from an AR(3) process with coefficients $\alpha_1 = 0.474$, $\alpha_2 = 1/4$ and $\alpha_3 = .241$ as follows. We can use the function `arima.sim` to create a random sample from an AR(p) model by providing the coefficients in a list from $\alpha_1, \ldots, \alpha_p$.

```{r}
set.seed(4840)
zz <- arima.sim(n = 300, model = list(ar = c(0.47368421, 1/4, 0.241)))
plot(zz)
```

Let's look at the correlogram. 

```{r}
acf(zz)
```

It is hard to tell what the order $p$ of the process is! For this, we would use the **partial autocorrelation function** which gives for each $k$ the correlation that results from removing any correlation from shorter lags. Let's look.

```{r}
pacf(zz)
```

Unlike `acf`, there is no value at lag 0, so we have 3 statistically significant values; hence, we guess that it is an AR(3) process (which it is). As a general rule, we would take $p$ to be the largest significant value. However, this can lead to some issues if we have significance at lag $k = 1, 2, 23$, for example, especially if there is no natural frequency of 23 in the data.

Once we have estimated the order of the AR process, we will want to estimate the coefficients. We saw how to do this directly when we have an AR(1) process. We will show two ways: the easy to understand but not perfect way, and the way using the built in R function.

First, let's recall the model:

\[
x_t = \alpha_1 x_{t - 1} + \alpha_2 x_{t - 2} + \alpha_3 x_{t - 3} + z_t
\]

Note that this is a linear regression model with predictors $x_{t - 1}, \ldots, x_{t - 3}$. So, we can use `lm` to estimate the coefficients as follows.

```{r}
x <- zz[4:300]
xt1 <- zz[3:299]
xt2 <- zz[2:298]
xt3 <- zz[1:297]
lm(x ~ xt1 + xt2 + xt3 - 1)
```

We see that the coefficients are not too far from the values that we used in creating the series.

For general AR($p$) processes, we simply run the R code without trying to understand exactly where it comes from. In the code below, we use `order = c(3, ,0, 0)` because we previously estimated $p = 3$. The two zeros should be ignored for now; whenever fitting an AR model we just put zeros in those positions.

```{r}
arima(zz, order = c(3, 0, 0))
```

We see that the estimates are $\hat \alpha_1 = .474$, $\hat \alpha_2 = .198$ and $\hat \alpha_3 = .2414$, which is pretty close to the true values of $\alpha_1 = 0.474$, $\alpha_2 = 1/4$ and $\alpha_3 = .241$. The intercept value means that R actually fit `zz - (-2.6326)` rather than `zz` when finding the coefficients. Compare with 

```{r}
arima(zz + 2.6326, order = c(3, 0, 0))
```

:::

::: example
Consider the high temperature at Lambert airport from 2018-2020, which can be obtained as follows.

```{r}
ss <- tswrdata::stltemp
library(tidyverse)
ss <- filter(ss, str_detect(name, "LAMBERT")) %>% 
  filter(str_detect(date, "2018|2019|2020"))
plot(ss$tmax)
```

This weather data is not consistent with an AR(p) process directly, as it does not appear to be stationary. We fit a trend to the data using `loess`, and then we ask about the residuals of that model.

```{r}
tmax <- ss$tmax
time <- 1:length(tmax)
mod <- loess(tmax ~ time, span = .3)
plot(mod)
points(mod$fitted, type = "l", col = 2)
```

We experiment with the `span` parameter untill we are satisfied that the curve is following the trend of the prices throughout the time period.

Now, let's look at the residuals.

```{r}
plot(mod$residuals, type = "l")
```

This could be an AR process. If we look carefully, though, then we can see that the variance in the residuals seems to be smaller in the summers than in the rest of the year. 

```{r}
acf(mod$residuals)
pacf(mod$residuals)
```

This could be an AR process of order 3 because the first 3 lags are significant.

```{r}
mod2 <- arima(as.vector(mod$residuals), order = c(3, 0, 0))
plot(residuals(mod2))
acf(resid(mod2))
forecast::checkresiduals(mod2)
```

The residuals do seem to be pretty well modeled by an AR(3) process, but I am not completely satisfied as the residuals seem to have lower variance in the summers than in the other seasons. 
:::

## Some Examples

This section consists entirely of examples.

::: example
Consider the daily USD-ZAR exchange rate data in `tswrdata`. This time we only consider the first 1000 entries of the data, because we saw previously that the variance of the residuals was approximately constant on that interval.

```{r}
zz <- tswrdata::samarket$zar_usd
zz <- zz[!is.na(zz)]
zz <- zz[1:1000]
plot(zz, type = "l")
acf(zz) 
```

Whoa, dude the autocorrelations are large!

```{r}
pacf(zz)
```

But, only the first lag component is significant according to `pacf`. Let's build the model.

```{r}
mod <- arima(zz, order = c(1, 0, 0))
plot(mod$residuals)
```

This looks reasonable.

```{r}
forecast::checkresiduals(mod)
```

All of this looks pretty good, so absent other information we would tend to believe that this is a good model. However, we **do** have other information that the variance does not stay constant over time, so it would be inappropriate to use this model for future predictions.
:::

::: example
Consider the global temperature series in the `tswrdata` package. We again average the two sources for each year.

```{r}
gg <- tswrdata::yearly
gg <- gg %>% 
  group_by(year) %>% 
  summarize(temp = mean(mean))
plot(gg$temp, type = "l")

acf(gg$temp)
```

This is plausibly consistent with an AR process.

```{r}
pacf(gg$temp)
```

The `pacf` suggests that we should try an AR(3) model. The `arima` command below gives an error, so I have to wrap it inside of `try` in order to get the book to compile! You don't need to do that yourself when running it interactively.

```{r}
mod <- try(arima(gg$temp, order = c(3, 0, 0)))
```

Hmmm, what a minute, `arima` says that we have a non-stationary AR part. That would mean the data is **not** consistent with an AR process, at least the types that we have been fitting so far. If we want to allow R to try to fit non-stationary models to the data, then we need to do some work.

```{r}
mod <- ar.ols(gg$temp)
mod
```

The function `ar.ols` allows the possibility of a non-stationary AR process. The best fit is order 6, which is quite a bit larger than what we were estimating from the `acf` plot.

```{r}
forecast::checkresiduals(mod)
```

::: alert
It is tempting to get around the non-stationary AR error message by switching `method = "ML"` or by using `ar` directly on the time series and ignoring `arima`. Please do not do this, because these functions will return stationary models even when the data indicates non-stationarity.
:::

This looks pretty good. Perhaps we can see whether the model is stationary.

```{r}
abs(polyroot(z = c(-1, 0.7022,  -0.0338,   0.0799,   0.2808,  -0.1752,   0.1768)))
```

The model is non-stationary, which matches what the `arima` error was indicating to us.

In summary, it appears that a **non-stationary** AR(6) process sufficiently explains the global temperature data. Absent other information, we would probably be satisfied with this random model.

The final model for this data is:

\[
x_t =  0.7022 x_{t - 1} - 0.0338 x_{t - 2} +  0.0799x_{t - 3}  - 0.2808x_{t - 4}  - 0.1752x_{t - 5} +  0.1768 x_{t - 6} + z_t
\]
where $z_t$ is iid normal with mean 0 and $\sigma^2 = 0.00902$. Let's simulate data from this AR process and see if it looks consistent with what the global temperature data looked like. We do not want to have a long burn-in period for this data; we start with the initial global temperature values.

```{r}
z <- rnorm(150, 0, sd = sqrt(0.00902))
x <- gg$temp[1:6]
for(i in 7:150) x[i] <-  0.7022 * x[i - 1] - 0.0338 * x[i - 2]  + 0.0799 * x[i - 3]  +  0.2808 * x[i - 4] - 0.1752 * x[i - 5] + 0.1768 *x[i - 6] + z[i]
plot(x, type = "l")
```

If you repeat the above code many times, you will see that data generated from this process tends to have long strings of increasing or decreasing values. 
:::


::: example
Let's consider the global temperature data once more.

```{r}
gg <- tswrdata::yearly
gg <- gg %>% 
  group_by(year) %>% 
  summarize(temp = mean(mean))
plot(gg$temp, type = "l")
```

We would certainly be forgiven for believing that this data comes from a **non-stationary** process. How can we tell whether data is stationary??

Well, there are some tests that can help us out.

```{r}
library(urca)
urca::ur.kpss(gg$temp) %>% summary()
```

The test statistic 2.557 is much larger than the critical value of .463 at the $\alpha = .05$ level, therefore we reject the null hypothesis that the data is stationary. We conclude that it is non-stationary. 

We can take differences to try to improve the situation.

```{r}
urca::ur.kpss(diff(gg$temp)) %>% summary()
```

Indeed, now we see that the data **is** stationary. We can then proceed to model the differences.

```{r}
dd <- diff(gg$temp)
plot(dd, type = "l")
acf(dd)
pacf(dd)
```

The differences seem to be plausibly AR(3)?  

```{r}
mod <- arima(dd, order = c(3, 0, 0))
forecast::checkresiduals(mod)
```

This looks pretty good, and `ar` also suggests a model of order 3.

```{r}
mod2 <- ar(dd)
mod2
```

Our model is:

\[
x_t - x_{t - 1} = -0.2989 (x_{t - 1} - x{t - 2}) - 0.2961 (x_{t - 2} - x_{t - 3}) - 0.2128 (x_{t - 3} - x_{t - 4}) + z_t
\]

If we wanted to, we could rewrite this for $x_t$ in terms of all of the previous values. 
:::


::: example
Let's simulate a time series of length 1000 for the following model:

\[
x_t = \frac 56 x_{t - 1} - \frac 16 x_{t - 2} + w_t
\]

```{r}
dd <- arima.sim(model = list(ar = c(5/6, -1/6)), n = 1000)
plot(dd, type = "l")

#' cf

z <- rnorm(10000)
x <- z[1:2]
for(i in 3:10000) x[i] <- 5 / 6 * x[i - 1] - 1/6 * x[i - 2] + z[i]
dd2 <- x[9001:10000]
plot(dd2, type = "l")
```

The correlogram and partial correlogram look like this:

```{r}
acf(dd)
pacf(dd)
```

It seems we have two significant values, as we would hope. We  note that the model is stationary, and the data also passes the test for starionarity.

```{r}
abs(polyroot(z = c(-1, 5/6, -1/6)))
urca::ur.kpss(dd) %>% summary()
```

We fit an AR model to the data.

```{r}
mod <- arima(dd, order = c(2, 0, 0), include.mean = F)
mod

#cf

mod2 <- ar(dd)
mod2
```

The confidence interval for the coefficients is given by

```{r}
sqrt(diag(mod$var.coef)) #these are the standard errors 

mod$coef + 2 * sqrt(diag(mod$var.coef)) #upper confidence bounds
mod$coef - 2 * sqrt(diag(mod$var.coef)) #lower confidence bounds
```

We compare the confidence intervals for the above to the true values of 5/6 and -1/6. Finally, we check the residuals.

```{r}
forecast::checkresiduals(mod)
```

:::

## Exercises

::: {.exercise #ex:rwalks} 
Consider the following data sets in the `fma` package, and indicate whether the data is consistent with a random walk model with no drift, a random walk with drift model, or neither.

a. The `beer` data set gives the monthly Australian beer production from Jan 1991 - Aug 1995.
b. The `bicoal` data set gives the annual bituminous coal production the USA from 1920-1968.
c. The `chicken` data set gives the price of chicken in US (constant dollars) from 1924-1993.
d. The `cowtemp` data gives the daily morning temperature of a cow.
:::


::: {.solution #hide:rwalks}
a. No, the differences are significant at 1 year intervals.
b. The `plot`, `acf` and `t.test` of the differences seem consistent with a random walk with no drift, even though the differences are significant at lag 2.
c. The `plot`, `acf` and `t.test` of the differences seem consistent with a random walk with no drift.
d. No, the differences are significant at lag 1.
:::


::: {.exercise #ex:GST}
Consider the adjusted closing cost of General Electric (symbol GE) in the calendar year 2018, which you can obtain using the `quantmod` package. Is the adjusted closing stock price consistent with a random walk with drift? If so, give a 95 percent confidence interval for the value of the drift.
:::

::: {.solution #hide:GST}

```{r}
library(quantmod)
getSymbols("GE")
GE <- GE[str_detect(index(GE), "2018")]
plot(GE$GE.Adjusted)
acf(diff(GE$GE.Adjusted), na.action = na.pass)
t.test(as.vector(diff(GE$GE.Adjusted)))

```

Yes, this seems to be consistent with a random walk with drift.

A 95 percent CI for the drift is $[-0.073, -0.003]$.

:::

::: {.exercise #ex:arpstationary}
Consider the following AR($p$) processes, and determine whether they are stationary or not by finding the roots of the associated polynomial.

a. AR(3) with coefficients $\alpha_1 = .0526, \alpha_2 = .833, \alpha_3 = -.448$.
b. AR(3) with coefficients $\alpha_1 = .474, \alpha_2 = .417, \alpha_3 = -.517$.
:::

::: {.solution #hide:arpstationary}
a. non-stationary
```{r}
alpha_1 = .0526
alpha_2 = .833
alpha_3 = -.448
abs(polyroot(z = c(-1, alpha_1, alpha_2, alpha_3))) 
```


b. Stationary
```{r}
alpha_1 = .474
alpha_2 = .417
alpha_3 = -.517
abs(polyroot(z = c(-1, alpha_1, alpha_2, alpha_3))) 
```
:::

```{r eval = FALSE, echo = FALSE}
fn <- function(x) {
  alpha_3 * x^3 + alpha_2 * x^2 + alpha_1 * x - 1
}
alpha_2 <- .85
abs(polyroot(z = c(1, alpha_1, alpha_2)))
ee <- sapply(seq(-1, 1, length.out = 30), function(alpha_3) {
  lapply(seq(-1, 1, length.out = 20), function(alpha_1) {
  lapply(seq(-1, 1, length.out = 25), function(alpha_2) {
    list(alpha1 = alpha_1, 
         alpha2 = alpha_2, 
         alpha3 = alpha_3,
         roots = min(abs(polyroot(z = c(-1, alpha_1, alpha_2, alpha_3))))
         )
  })
})
})
bind_rows(ee)
ee <- bind_rows(ee)
mutate(ee, sum = abs(alpha3) + abs(alpha1) + abs(alpha2)) %>% 
  filter(abs(roots - 1) > .02)%>% 
  distinct(roots, .keep_all = T) %>% 
  arrange(abs(roots - 1)) %>% 
  print(n = 30)
```

::: {.exercise #ex:stationaryplots}
Consider the three time series plotted below. Determine whether the series is likely stationary or likely non-stationary.

```{r echo=FALSE}
alpha_1 = .474
alpha_2 = .417
alpha_3 = -.517
plot(arima.sim(model = list(ar = c(alpha_1, alpha_2, alpha_3)), n = 400), 
     type = "l", 
     ylab = "Value",
     main = "Plot 1")
```

```{r echo=FALSE}
alpha_1 = .474
alpha_2 = .417
alpha_3 = -.517
plot(arima.sim(model = list(ar = c(alpha_1, alpha_2, alpha_3)), n = 400), 
     type = "l", 
     ylab = "Value",
     main = "Plot 2")
```

```{r echo=FALSE}
alpha_1 = .0526
alpha_2 = .833
alpha_3 = -.448
x <- 1
x[2] <- 2
x[3] <- rnorm(1)
z <- rnorm(400, 0, 1)
for(i in 4:400) x[i] <- alpha_1 * x[i - 1] + alpha_2 * x[i - 2] + alpha_3 * x[i - 3] + z[i]
plot(x[1:100], type = "l", ylab = "Value", main = "Plot 3", xlab = "Time")
```
:::

::: {.solution #hide:stationaryplots}
Plots 1 and 2 are stationary, while plot 3 is not.
:::

::: {.exercise #ex:simdata}
Consider the data set `sim_data_1` in the `tswrdata` package. 

a. Plot the time series and the correlogram and determine whether the time series appears consistent with an AR process.
b. Estimate the order of the AR process and estimate the coefficients.
c. Examine the residuals and determine whether the AR model is satisfactory for the data set.
:::

::: {.solution #hide:simdata}

a. Plausibly consistent with AR model.
```{r}
zz <- tswrdata::sim_data_1
plot(zz)
acf(zz)
```

b. 
```{r}
pacf(zz)
```

We will try order 2.

```{r}
mod1 <- arima(zz, order = c(2, 0, 0))
acf(residuals(mod1))
```

c. This looks pretty good, even though there are 4 lags that are significant. The probability of 4 or more lags that are significant is about 3 percent, which means that there may be something more going on, even though I know there isn't because I know how the data was created!

```{r}
sum(dbinom(4:24, size = 24, prob = .05))
```

:::

::: {.exercise #ex:spyar}
Use the `quantmod` package to access the adjusted daily closing price of `SPY` for the year 2020.

a. Is the data consistent with a random walk model with drift?

b. Is the data consistent with an AR(p) model?
:::

::: {.solution #hide:spyar}
a. 

```{r}
quantmod::getSymbols("SPY")
library(zoo)
library(stringr)
SPY <- SPY[str_detect(index(SPY), "2020")]
plot(SPY$SPY.Adjusted)
```

This does not look like a typical random walk with drift. If it were only from April through December, then perhaps. Let's continue looking.

```{r}
plot(diff(SPY$SPY.Adjusted))
```

The variance of the differences at lag 1 seem to depend on time.

```{r}
acf(diff(SPY$SPY.Adjusted), na.action = na.pass)
```

There seems to be quite a bit of autocorrelation. My overall answer is that this data is **not** consistent with a random walk with drift model.

b. 

```{r}
plot(SPY$SPY.Adjusted, type = "l")
```

This could be an AR(p) process, I suppose, though it isn't what they typically look like. It seems like it might be a non-stationary AR(p) process.

```{r}
library(urca)
summary(urca::ur.kpss(SPY$SPY.Adjusted)) #non-stationary!

acf(SPY$SPY.Adjusted)
```

This correlogram is consistent with an AR(p) process with high autocorrelation, but it is also consistent with a time series whose level is changing over time, which I suspect is what this is.

```{r}
pacf(SPY$SPY.Adjusted)
```

```{r}
mod <- arima(SPY$SPY.Adjusted, order = c(3, 0, 0))
acf(residuals(mod))
plot(residuals(mod))
```

This data is not consistent with a stationary AR process. Let's see about non-stationary.

```{r}
mod2 <- ar.ols(SPY$SPY.Adjusted)
acf(mod2$resid, na.action = na.pass)
plot(mod2$resid, type = "l")
```

This is pretty good, except the variance seems to be different at different times. Since we have observed this behavior in other financial models, it would be wise to investigate further.

:::

::: {.exercise #ex:ausbeermod}
Consisder the `fma::beer` data set. 

a. Use `decompose` to decompose the data into trend, seasonal and random components. 
b. Examine the residuals of the model from part a. Do they appear to be consistent with white noise?
:::

::: {.solution #hide:ausbeermod}

a. The `beer` data has frequency 12 by default, which seems good.

```{r}
bb <- fma::beer
plot(bb)
mod <- decompose(bb)
plot(mod)
```

This looks reasonable. Let's examine the residuals of the model.

```{r}
forecast::checkresiduals(mod$random)
```

Despite there being no degrees of freedom known to run a test on the residuals, this model appears to adequately describe the beer production. There are no red flags with the residuals.

:::



::: {.solution #hide:tempar}

```{r}
ss <- tswrdata::stltemp
library(tidyverse)
ss <- filter(ss, str_detect(name, "LAMBERT")) %>% 
  filter(str_detect(date, "200[0-5]"))
plot(ss$tmin)

tmin <- ss$tmin
time <- 1:length(tmin)
mod <- loess(tmin ~ time, span = .15)
plot(mod)
points(mod$fitted, type = "l", col = 2)
```

```{r}
plot(mod$residuals)
acf(mod$residuals)
pacf(mod$residuals)

mod2 <- arima(mod$residuals, order = c(3, 0, 0))
forecast::checkresiduals(mod2)
```

The residuals of the `loess` model are reasonably well modeled by an AR(3) process, except that there is perhaps a seasonality to the variance of the residuals. The summer residuals seem to have lower variance than the rest of the residuals.

```{r}
plot(mod$fitted, type = "l", ylim = c(-30, 75))
points(residuals(mod))

```


:::


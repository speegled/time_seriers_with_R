# Non-Stationary Models {#nonstationarymodels}

```{r child = 'pre-chapter-script.Rmd'} 
```

## ARIMA models {#arimamodels}

In the previous chapter, we introduced ARMA(p, q) models, which are flexible models for modeling stationary processes. Now, we introduce ARIMA models, which are autoregressive integrated moving average models. These models, and their seasonal variants, are an even more flexible class of models, and can be used to fit a wide range of data. That flexibility comes at a cost, though, in terms of complexity to fit models as well as in interpretation. 

We start by explaining what the *integrated* part of an ARIMA model means. This means that we **difference** the data 1 or more times. Differencing a time series will remove a linear trend from the time series, and it will also convert a random walk into white noise. Let's remind ourselves why.

If $x_t$ is a random walk, then $x_t = x_{t - 1} + w_t$, where $w_t$ is white noise. We simply solve for $x_{t} - x_{t - 1}$ and see that the differenced time series is white noise. If $x_t$ has a linear trend, then $x_t = a + bt + w_t$, where $w_t$ is white noise. In this case, $x_{t - 1} = a + bt - b + w_{t - 1}$, so we compute
\begin{align*}
x_t - x_{t - 1} &= a + bt + w_t - a - bt + b - w_{t - 1} \\
&=b + w_t - w_{t - 1}
\end{align*}
So, we see that the differenced time series is a MA(1) process with mean $b$.

Let's verify both of the previous claims via simulations. We start with white noise.

```{r}
x <- cumsum(rnorm(100))
plot(x)
y <- diff(x)
plot(y)
acf(y)
```

Indeed, the differenced time series seems to be white noise. Now let's look at a linear trend.

```{r}
a <- 1
b <- 2
t <- 1:100
x <- a + b * t + rnorm(100)
plot(t, x)

y <- diff(x)
plot(y) #this should be MA with mean b
arima(y, order = c(0, 0, 1), include.mean = T)
```

The value of the intercept is very close to $b$, and the coefficient is close to the correct value of $-1$. We check that the residual of **this** model is white noise.

```{r}
acf(resid(arima(y, order = c(0, 0, 1), include.mean = T)))
```

In general, if you have a trend that is polynomial of degree $n$, then taking $n$ differences will make the series a stationary MA(n) model. See, for example, \@ref(ex:squarediff). However, many times in simulations it can be hard to distinguish how many differences we need to include if we only use `acf` plots. Here is an example of a quadratic.

```{r}
t <- seq(0, 1, length.out = 1000)
a <- 1 
b <- -1
d <- 2
x <- a  + b * t + d * t^2 + rnorm(1000, 0, .05)
plot(x)

y <- diff(x, d = 1) 
acf(resid(arima(y, order = c(0, 0, 1))))
```

The acf plot looks pretty much like white noise when we (incorrectly) only include one difference. Including a second difference does not seem to improve the situation.

```{r}
mod_1 <- arima(y, order = c(0, 0, 1))
y2 <- diff(x, d = 2)
mod_2 <- arima(y2, order = c(0, 0, 2))
acf(resid(mod_2))
```

Therefore, at least from a theoretical point of view, we might like to use AIC to decide between two models of this type. Unfortunately, we can only use AIC when the underlying data is identical. In the set-up above, we are using two related, but different data sets.

To get around this, we include the integrated term directly in the order specification.

```{r}
mod_1 <- arima(x, order = c(0, 1, 1))
AIC(mod_1)
mod_2 <- arima(x, order = c(0, 2, 2))
AIC(mod_2)
```

Note that the quadratic model is preferred over the linear model in this case. However, if the quadratic term is smaller, we will often prefer the linear model. Let's confirm that going to a cubic model would be too much.

```{r}
mod_3 <- arima(x, order = c(0, 3, 3))
AIC(mod_3)
```

However, when we correctly include two differencing terms, the AIC decreases. Going to a third difference typically makes the AIC increase again, as above. Let's check all of this out via a large simulations.

```{r cache = TRUE}
library(future.apply) #for parallel
plan(multiprocess, workers = 6)
aic_data <- future_replicate(200, {
  x <- a  + b * t + d * t^2 + rnorm(1000, 0, .05)
  mod_1 <- arima(x, order = c(0, 1, 1))
  mod_2 <- arima(x, order = c(0, 2, 2))
  mod_3 <- arima(x, order = c(0, 3, 3))
  c(AIC(mod_1), AIC(mod_2), AIC(mod_3))
})
aic_data <- t(aic_data)
mean(aic_data[,1] < aic_data[,2]) #percent of times we choose linear
mean(aic_data[,3] < aic_data[,2]) #percent of times we choose cubic

```

That seems pretty good. Let's for kicks compare to what would have happened if we would have used `AIC` on the differenced data sets.

```{r cache = TRUE}
plan(multiprocess, workers = 6)
dd2 <- future_replicate(1000, {
  x <- a  + b * t + d * t^2 + rnorm(1000, 0, .05)
  y2 <- diff(x, 2)
  y3 <- diff(x, 3)
  y1 <- diff(x, 1)
  mod_1 <- arima(y1, order = c(0, 0, 1))
  mod_2 <- arima(y2, order = c(0, 0, 2))
  mod_3 <- arima(y3, order = c(0, 0, 3))
  c(AIC(mod_1), AIC(mod_2), AIC(mod_3)) #DONT DO THIS
})
plan(sequential) #explicitly closes parallel processes
dd2 <- t(dd2)
mean(dd2[,1] < dd2[,2])
mean(dd2[,3] < dd2[,2])
```

Oof. That's terrible.

::: {.example #exm:ariprocess}
Write out the model ARIMA(1, 1, 0) with AR coefficient .2 and standard deviation of the underlying white noise process 2.

The model says that $x_t - x_{t - 1}$ is an ARMA(p, q) process with the given coefficients. Therefore, let $y_t = x_t - x_{t - 1}$.
\[
y_t = .2 y_{t - 1} + w_t
\]
where $w_t$ is white noise with standard deviation 2. In other words, solving for $x_t$, 
\[
x_t = 1.2 x_{t - 1} - .2 x_{t - 2} + w_t
\]

We can check our computation as follows:
```{r}
w <- rnorm(10000, 0, 2)
x <- w[1]
x[2] <- w[2]
for(i in 3:10000) {
  x[i] <- 1.2 * x[i - 1] - .2 * x[i - 2] + w[i]
}
arima(x[1001:10000], order = c(1, 1, 0))
```
:::

:::tryit
What would the equation be for an ARIMA(1, 1, 1) model with AR coefficient .5, MA coefficient .4 and standard deviation of the underlying white noise process 3?

Verify via simulations.
:::

::: example
Let's model the global temperature data using an ARIMA model. It can be kind of tricky to do this by hand! We first plot it and see that it looks like it might be an AR(p) model, but the residuals are not clean.

```{r}
tt <- filter(tswrdata::yearly, source == "GCAG")
tt <- arrange(tt, year)
plot(tt$mean, type = "l")
acf(tt$mean)
mod_1 <- arima(tt$mean, order = c(1, 0, 0))
acf(resid(mod_1))
```

So, we try a few other things, none of which seem to work great.

```{r}
mod_2 <- arima(tt$mean, order = c(1, 1, 0))
acf(resid(mod_2))
mod_3 <- arima(tt$mean, order = c(2, 0, 0))
acf(resid(mod_3))
mod_4 <- arima(tt$mean, order = c(2, 2, 0))
acf(resid(mod_4))
```

Let's write some code that will systematically go through all of the ARIMA models of certain orders, and compute the AIC.

```{r eval = FALSE}
orders <- expand.grid(0:2, 0:2, 0:2)
orders$AIC <- NA
for(i in 1:nrow(orders)) {
  mod <- arima(tt$mean, order = as.numeric(orders[i,1:3]))
  orders$AIC[i] <- AIC(mod)  
}
```

Uh-oh, we get an error here. What we want to happen is that when R finds an issue with the model, that AIC gets reported as `NA`. We can do this using `tryCatch`.

```{r}
orders <- expand.grid(0:2, 0:2, 0:2)
orders$AIC <- NA
for(i in 1:nrow(orders)) {
  orders$AIC[i] <- tryCatch(expr = {
    mod <- arima(tt$mean, order = as.numeric(orders[i,1:3]))
    AIC(mod)},
    error = function(e){NA})
}
arrange(orders, AIC)
```


We see that the best model by the AIC criterion is ARIMA(0, 1, 2). Let's check the residuals. Looks pretty good to me.

```{r}
mod <- arima(tt$mean, order = c(0, 1, 2))
acf(resid(mod))
mod
```
:::

:::tryit
Write out the equation that describes the yearly global temperature in the example above.
:::

We continue by simulating from a model like we just computed. Note that if we re-run this simulation several times, we see that the global temperatures are just as likely to decrease as they are to increase.

```{r}
plot(arima.sim(model = list(order = c(0, 1, 2), ma = c(-.2889, -.1996)), 
               n = 1000, 
               sd = sqrt(0.009284)))
```

Now, let's see how to simulate values for the 10 years after the end of the data set. We need to know what the estimates for the last two residual values are, as well as the last value of the data. We then add 10 randomly chosen residuals, forecast the future values with error and plot.

```{r}
w <- c(resid(mod)[136:137], rnorm(10, 0, sqrt(0.009284)))
x <- tt$mean[136:137]
for(i in 3:12) {
  x[i] <- x[i - 1] + w[i] - 0.2889 * w[i - 1] - 0.1996 * w[i - 2]
}
plot(c(tt$mean, x[3:12]))
```

If we repeat this a bunch of times, then we start to get a feel for the range of possible mean temperatures that are coming. The following creates a 10 by 100 matrix with possible mean temps for the next 10 years.

```{r}
poss_temps <- replicate(1000, {
  w <- c(resid(mod)[136:137], rnorm(10, 0, sqrt(0.009284)))
  for(i in 3:12) {
    x[i] <- x[i - 1] + w[i] - 0.2889 * w[i - 1] - 0.1996 * w[i - 2]
  }
  x[2:12]
})
```

```{r}
plot(x = tt$year, tt$mean, type = "l", xlim = c(1880, 2025), ylim = c(-0.5, 1.4))
for(i in 1:1000) {
  points(x = 2016:2026, y = poss_temps[,i], type = "l", col='#FF000004', lwd = 4)
}
```

```{r}
ttsmall <- filter(tt, year > 2000)
plot(x = ttsmall$year, ttsmall$mean, type = "l", xlim = c(2000, 2025), ylim = c(-0.5, 1.4))
for(i in 1:1000) {
  points(x = 2016:2026, y = poss_temps[,i], type = "l", col='#FF000004', lwd = 4)
}
```


The last two digits in the `col` item control the **transparency** of the additional lines. I like to make the lines very transparent, but put in a lot of them. Then where muptiple lines overlap, the overall plot gets darker and indicates a higher probability of occuring. I should point out that we have only taken into account **one source** of variation, which is the estimate of the model residuals. We also have errors associated with our **parameter estimates**, which should be taken into account to get a more accurate estimate of the errors associated with predictions. One start would be to select parameter estimates at random from, say, a 2 standard error interval around the point estimate of the parameter. Even this is not perfect, though, because there is most likely **correlation** between the parameter estimates as well. We could imagine that a decrease in the first MA coefficient could consistently lead to an increase in the second coefficient, for example. So, we leave it like this, even though it is far from perfect.

### Seasonal ARIMA Model

ARIMA models can also be generalized to cover time series with a seasonal component. If we difference the time series at lag equal to the frequency of the time series, then we can remove the seasonal component. Let's look at an example. 

```{r}
amtrak <- tswrdata::amtrak
plot(amtrak$num_passengers, type = "l") 
plot(diff(amtrak$num_passengers, lag = 12), type = "l")
```

The second plot seems to have at least some of the seasonality removed from the time series. A **seasonal** ARIMA model has all of the components of a regular ARIMA model as well as AR and MA components to a **seasonal** lag of order $D$. These models are sometimes hard to understand, so we will focus on building them and checking the residuals. See the notes for examples of how to interpret the parameters of the model.

::: example
Consider the electricity data in the `tswrdata` package.

```{r}
elec <- tswrdata::cbe$elec
elec <- ts(elec, frequency = 12)
plot(elec)
mod <- forecast::auto.arima(elec)
```

According to this, the electricity production can be modeled as an $ARIMA(1, 1, 2)(0, 1, 1)_{12}$ model. Yikes, that is a lot of parameters!!! Let's check the residuals and add a forecast to the electricity production plot, and call it good.

```{r}
forecast::checkresiduals(mod)
```

The residuals are still not stationary, so this model is not quite there. However, let's plot the forecast at the end of the time series, together with a 2 se error bar. Note that these errors may not be accurate because we have not yet completely specified the model.

```{r}
predictions <- predict(mod, n.ahead = 12)
plot(elec, xlim = c(1, 34), ylim = c(0, 18000))
points(seq(34, 35, length.out = 13), c(elec[length(elec)], predictions$pred), 
       type = "l", 
       col = 2)
points(seq(34, 35, length.out = 13), 
       c(elec[length(elec)], predictions$pred + 2 * predictions$se), 
       type = "l", 
       col = 3)
points(seq(34, 35, length.out = 13), 
       c(elec[length(elec)], predictions$pred - 2 * predictions$se), 
       type = "l", 
       col = 3)
```

:::

::: example

Let's consider the amtrak data set, and do the same thing, except this time we use the `forecast` function from the `forecast` package.

```{r}
amtrak <- tswrdata::amtrak$num_passengers
amtrak <- ts(amtrak, frequency = 12)
plot(amtrak)

mod <- forecast::auto.arima(amtrak, allowdrift = T)
forecast::checkresiduals(mod)
```

```{r}
predictions <- forecast::forecast(mod)
plot(predictions)
```

:::

## ARCH and GARCH

We have seen **many** instances where everything looks good up until we plot the residuals, and darn it, the variance of the residuals does not appear to be constant. 

::: example
Let's consider the USD-ZAR exchange rate data.

```{r}
aa <- tswrdata::samarket$zar_usd
plot(aa, type = "l")
```

What type of model would you think for this? Well, let's try a non-seasonal ARIMA that minimizes AIC. You should check that `auto.arima` doesn't do very well on this data set with the default settings.

```{r}
orders <- expand.grid(0:2, 0:2, 0:2)
orders$AIC <- NA
for(i in 1:nrow(orders)) {
  tryCatch(expr = {mod <- arima(aa, order = as.numeric(orders[i,1:3]))
  orders$AIC[i] <- AIC(mod)},
  error = function(e){NA})
}
arrange(orders, AIC)
```

We get an ARIMA(1, 1, 1) model. Let's examine the residuals.

```{r}
mod <- arima(aa, order = c(1, 1, 1))
forecast::checkresiduals(mod)
```
These residuals have a couple of problems. First, there is autocorrelation at large lags. That doesn't seem consistent with white noise. The second problem is that the variance of the residuals does not seem constant, but rather goes through periods of larger and smaller values. The values seem somewhat random.
:::

ARCH and GARCH models are used to model time series such as these. Our first question is: how do we recognize such a series? The correlograms should look like white noise (though these residuals perhaps do not). However, the correlogram of the **squared** values will be correlated.

```{r}
rr <- residuals(mod)
acf((rr - mean(rr, na.rm = T))^2, na.action = na.pass)
```

We see that the normalized squared values of the residuals are highly autocorrelated. This is common, and the kind of thing we want to be able to model with ARCH. The acronym ARCH stands for autoregressive conditional heteroskedastic. 

The time series $\epsilon_t$ follows an ARCH(p) process if
\[
\epsilon_t = w_t \sqrt{\alpha_0 + \sum \alpha_i \epsilon_i^2},
\]
which implies by Exercise \@ref(archvar) that 
\[
V(\epsilon_t) = \alpha_0 + \alpha_1 V(\epsilon_{t - 1}) + \cdots + \alpha_p V(\epsilon_{t - p}^2)
\]
How does this introduce the type of volatility that we have seen??

Let's simulate some data. Play around with the values of `alpha` to see what kind of impact it has.

```{r}
w <- rnorm(1000)
alpha <- .1
alpha1 <- .9
eps <- w[1]
for(i in 2:1000) eps[i] <- w[i] * sqrt(alpha + alpha1 * eps[i - 1]^2)
plot(eps, type = "l")
acf(eps)
acf(eps^2)
```

From this, we can see that the ARCH process introduces volatility in the time series. 

:::aside
At this point, we are usually modeling the residuals of some other model. The purpose for doing this is often so that we can get better estimates for the errors in our predictions. If we are planning for a rare event, say looking for the worst 5 percent of returns in a stock market, it may be helpful to model the variance in this way, because volatility in variance can lead to more extreme highs and lows.
:::

::: tryit
Simulate 1000 values from an ARCH(2) model with coefficients $\alpha_0 = .2$, $\alpha_1 = .3$ and $\alpha_2 = .4$. Examine the acf plot of the time series and of the squared values of the time series.
:::



If we examine the correlogram of the squared values of a mean zero time series and determine that the variance is autocorrelated, then we will wish to model these values. We can use the `garch` function in the `tseries` package as follows. This works well because we **know** that the data was created via an ARCH(2) process. 

:::alert
The `order` argument in `garch` feels backwards to me. The first component is the generalized part, and the second component is the autoregressive part. So, an ARCH(2) model should be `order = c(0, 2)`.
:::


```{r}
library(tseries)
mod <- tseries::garch(eps, order = c(0, 2))
AIC(mod)
forecast::checkresiduals(mod)
```

All of this looks good. 

### GARCH models

Moving on, we consider GARCH(q, p) models. The time series $\epsilon_t$ is said to follow a GARCH(p, q) process if
\[
\epsilon_t = w_t \sqrt{h_t},
\]
where
\[
h_t = \alpha_0 + \sum_{i = 1}^p \alpha_i \epsilon_{t - i}^2 + \sum_{j = 1}^q \beta_i h_{t - j}
\]

Let's start by simulating data from this process.

```{r}
N <- 5000
w <- rnorm(N)
h <- rep(0, N)
eps <- rep(0, N)
alpha0 <- .2
alpha1 <- .3
beta1 <- .5
for(i in 2:N) {
  h[i] <- alpha0 + alpha1 * eps[i - 1]^2 + beta1 * h[i - 1]
  eps[i] <- w[i] * sqrt(h[i])
}
eps <- eps - mean(eps)
plot(eps, type = "l")
acf(eps)
acf(eps^2)
```

We can see that the series appears to have correlated variances. Let's pretend that we don't know that it is a GARCH(1, 1) process, and we fit a bunch of models to it and choose the one with lowest AIC. In this case, we have to have quite a large sample size before the AIC is able to correctly identify the model, and even still we see tht the AIC is not that much smaller than the AIC of some of the other models.

```{r}
orders <- expand.grid(0:3, 0:3)
orders$AIC <- NA
for(x  in 1:nrow(orders)) {
  orders$AIC[x] <- tryCatch(AIC(garch(eps[101:5000], 
                                      order = as.numeric(orders[x,]), 
                                      control = garch.control(trace = FALSE))),
           error = function(e) {NA})
}
arrange(orders, AIC)
```

:::tryit
Create a time series of length 5000 that follows a GARCH(2, 1) model with $\alpha_0 = .2$, $\alpha_1 = .3$, $\alpha_2 = .4$ and $\beta_1 = .4$. Use the method above to fit a GARCH(q, p) model to it. Does it get $p$ and $q$ correct? 
:::

::: {.example #exm:garchmodel}
Return to the exchange rate example that we were looking at earlier in the section. 

Create the plot below by restricting to data from 2018 on.

```{r echo=FALSE}
ee <- tswrdata::samarket
eesmall <- filter(ee, lubridate::year(date) > 2017) %>% 
  filter(!is.na(zar_usd))
ggplot(eesmall, aes(x = date, y = zar_usd)) + 
  geom_point()
```

Use `arima.auto` to confirm that the following ARIMA model best describes the data according to AIC criterion.

```{r echo=FALSE, message=F,warning=F}
library(forecast)
eesmall <- arrange(eesmall, date)

mod <- auto.arima(eesmall$zar_usd)
#mod <- arima(eesmall$zar_usd, order = c(0, 1, 0), method = "CSS")
```

Check the residuals.

Write out the full model.

Examine the variance of the residuals using a correlogram, and see that they appear correlated.

Model the residuals using a GARCH model. Looks like we need a GARCH(1, 1) model for the residuals.

```{r, echo = FALSE}
orders <- expand.grid(0:3, 0:3)
orders$AIC <- NA
library(tseries)
for(x  in 1:nrow(orders)) {
  orders$AIC[x] <- tryCatch(AIC(garch(residuals(mod), order = as.numeric(orders[x,]),
                                      control = garch.control(trace = FALSE))),
           error = function(e) {NA})
}
arrange(orders, AIC)
mod2 <- garch(residuals(mod), order = c(1, 1), control = garch.control(trace = FALSE))
summary(mod2)
```

Let's check the residuals of **this** part! It seems we have adequately modeled the correlation in the variance.

```{r echo=FALSE}
acf(residuals(mod2)^2, na.action = na.pass)
```
:::


::: {.example #exm:garcherrorsim}
In this example, we investigate obtaining error bounds for future values of the ZAR-USD exchange rate. We recall that we modeled the time series as a random walk process, and the residuals as a GARCH(1,1) process.

```{r echo=FALSE}
ee <- tswrdata::samarket
eesmall <- filter(ee, lubridate::year(date) > 2017) %>% 
  filter(!is.na(zar_usd))
eesmall <- arrange(eesmall, date)

mod <- auto.arima(eesmall$zar_usd)
```


```{r echo=FALSE}
ggplot(eesmall, aes(x = date, y = zar_usd)) + 
  geom_line()
```

We wish to give bounds for the ZAR-USD exchange rate for the 30 days beyond the end of the time series that we have. This is a particularly challenging assigment because we know that there was **severe** market turbulence in many markets around the middle of March, 2020. 

Simulate data from March 18, 2020 through April 18, 2020 from the ZAR-USD exchange rate assuming that it is exactly a random walk with the parameters given in your model and plot. If you `set.seed(4840)` you should get similar answers to what I have below. Note that there are only 23 business days in this time period. You can create a vector with the correct dates in it by using

```{r}
library(lubridate)
dd <- seq(ymd("2020-03-18"), ymd("2020-04-18"), by = 1)
dd <- dd[wday(dd) %in% 2:6]
dd
```


```{r echo=FALSE}
set.seed(4840)
new_vals <- arima.sim(model = list(order = c(0, 1, 0)), n = 23, sd = sqrt( 0.01879))[-1] + eesmall$zar_usd[553]
eesmall$color = "Known"
new_dat <- data.frame(date = dd, zar_usd = new_vals, sa40 = NA, color = "Simulated")

eesmall2 <- rbind(eesmall, new_dat)

ggplot(eesmall2, aes(x = date, y = zar_usd, color = factor(color))) + 
  geom_line() +
  labs(color = "time")
```

Now let's simulate $x_t = x_{t - 1} + \epsilon_t$, where $\epsilon_t$ come from the GARCH(1, 1) model that we built in the previous example. Ideally we should use the residuals from the data as a seed for the simulation, but we will forgo that step for simplicity.

```{r echo=FALSE}
x <- eesmall$zar_usd[553]
eps <- TSA::garch.sim(alpha = c(0.0004362,0.0550778), beta = 0.9254048, n = 50)
for(i in 2:24) {
  x[i] <- x[i - 1] + eps[i]
}
new_dat <- data.frame(date = dd, zar_usd = x[-1], sa40 = NA, color = "Simulated")
eesmall2 <- rbind(eesmall, new_dat)

ggplot(eesmall2, aes(x = date, y = zar_usd, color = factor(color))) + 
  geom_line() +
  labs(color = "time")
```

It is difficult to see much difference in these for a single run, but we would hope that the second one would be a more accurate simulation of possible future values. Now, repeat the simulation using the GARCH(1,1) model 1,000 times and store in a matrix with 23 columns and 1000 rows. Then use `quantile` on each column to get an approximate 95 percent confidence interval for the lower and upper bounds of the exchange rate over the time period. Plot your results. 

```{r echo=FALSE}
N <- 1000
mats <- matrix(rep(0, 23 * N), ncol = 23)
for(j in 1:N) {
  x <- eesmall$zar_usd[553]
  eps <- TSA::garch.sim(alpha = c(0.0004362,0.0550778), beta = 0.9254048, n = 50)
  for(i in 2:24) {
    x[i] <- x[i - 1] + eps[i]
  }
  mats[j,] <- x[-1]
}
lower <- data.frame(lower = apply(mats, 2, function(x) quantile(x, .025)), date = dd, color = "lower")
upper <- data.frame(upper = apply(mats, 2, function(x) quantile(x, .975)), date = dd, color = "upper")
ggplot(eesmall2, aes(x = date, y = zar_usd, color = factor(color))) + 
  geom_line() +
  labs(color = "time") +
  geom_line(data = lower, mapping = aes(x = date, y = lower)) +
  geom_line(data = upper, mapping = aes(x = date, y = upper))
```

Use the true data that I sent and compute the true percentage of times the exchange rate was in the 95 percent confidence interval. I got about 35 percent.

```{r echo=FALSE}
library(here)
here::i_am("time_series_with_R.Rproj")
tt <- read.csv(here("data/exchange"), header = FALSE)
mean(tt$V1 > lower$lower & tt$V1 < upper$upper)
```

Why do you think our model failed to provide accurate confidence intervals for the exchange rate?
:::


## Exercises

::: {.exercise #ex:ex1}
Simulate an ARCH(2) process with your choice of parameters, and plot the correlogram of the time series and of the squares of the values of the time series.
:::


::: {.exercise #ex:ex2}
For the two correlograms of the squares of values given below, which is consistent with an ARCH(p) process?

```{r echo=FALSE}
x <- rnorm(1000)

eps <- c(0, 0)
a0 <- .3
a2 <- .4
a1 <- .3

for(i in 3:1000) {
  eps[i] <- x[i] * sqrt(a0 + a1 * eps[i - 1]^2 + a2 * eps[i - 2]^2)
}

acf(x^2, main = "plot 1")
acf(eps^2, main = "plot 2")
```
:::


::: {.exercise #ex:ex3}
Simulate a GARCH(1, 1) process with $\alpha_0 = 0.2$, $\alpha_1 = 0.4$ and $\beta_1 = 0.3$, plot the data and the acf of the sqaured values of the time series.
:::

::: {.exercise #ex:ex4}
Fill in the details in Example \@ref(exm:garchmodel).
:::

::: {.solution #hide:garchmodel}
Return to the exchange rate example that we were looking at earlier in the section. 

Create the plot below by restricting to data from 2018 on.

```{r echo=TRUE}
ee <- tswrdata::samarket
eesmall <- filter(ee, lubridate::year(date) > 2017) %>% 
  filter(!is.na(zar_usd))
ggplot(eesmall, aes(x = date, y = zar_usd)) + 
  geom_point()
```

Use `arima.auto` to confirm that the following ARIMA model best describes the data according to AIC criterion.

```{r echo=TRUE, message=F,warning=F}
library(forecast)
eesmall <- arrange(eesmall, date)

mod <- auto.arima(eesmall$zar_usd)
summary(mod)
```

Check the residuals.

Write out the full model.

Examine the variance of the residuals using a correlogram, and see that they appear correlated.

Model the residuals using a GARCH model. Looks like we need a GARCH(1, 1) model for the residuals.

```{r, echo = TRUE}
orders <- expand.grid(0:3, 0:3)
orders$AIC <- NA
library(tseries)
for(x  in 1:nrow(orders)) {
  orders$AIC[x] <- tryCatch(AIC(garch(residuals(mod), order = as.numeric(orders[x,]),
                                      control = garch.control(trace = FALSE))),
           error = function(e) {NA})
}
arrange(orders, AIC)
mod2 <- garch(residuals(mod), order = c(1, 1), control = garch.control(trace = FALSE))
summary(mod2)
```

Let's check the residuals of **this** part! It seems we have adequately modeled the correlation in the variance.

```{r echo=TRUE}
acf(residuals(mod2)^2, na.action = na.pass)
```
:::




::: {.exercise #ex:squarediff}
Suppose $x_t = a + bt + ct^2 + w_t$. What kind of series is the **twice differenced** time series? That is, let $y_t = x_t - x_{t - 1}$ and $u_t = y_t - y_{t - 1}$; I am asking about $u_t$.
:::



::: {.exercise #maeq}
Suppose that $x_t$ is an ARIMA(0, 1, 1) model with MA coefficient .3 and standard deviation of the white noise process 1. Find an equation for $x_t$, as was done for the ARIMA(1, 1, 0) model in Example \@ref(ariprocess).
:::


